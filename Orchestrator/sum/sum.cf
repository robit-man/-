model_name=llama3.2:1b
input_mode=port
output_mode=port
input_format=streaming
output_format=streaming
port_range=6200-6300
orchestrator_host=localhost
orchestrator_ports=6000-6005
route=/slm
script_uuid=a13d2700-7b27-4224-9ac3-63437c7ba781
system_prompt="You are a summary agent, the following content must be taken in and crafted into an appropriate concise response that will be presented back to the user who initiated this multi-agent session, please know that the following content is a set of both original user message that should be labeled, as well as the main llm response to it: "
temperature=0.7
top_p=0.9
max_tokens=2048
repeat_penalty=1.0
inference_timeout=5
json_filtering=true
port=6202
data_port=6001
