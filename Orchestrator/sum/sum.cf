model_name=llama3.2:1b
input_mode=port
output_mode=port
input_format=streaming
output_format=streaming
port_range=6200-6300
orchestrator_host=localhost
orchestrator_ports=6000-6005
route=/slm
script_uuid=a13d2700-7b27-4224-9ac3-63437c7ba781
system_prompt="Summarize and combine input into a simple conversational response"
temperature=0.7
top_p=0.9
max_tokens=150
repeat_penalty=1.0
inference_timeout=5
json_filtering=true
port=6202
data_port=6001
