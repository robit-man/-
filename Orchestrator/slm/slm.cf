model_name=llama3.2:1b
input_mode=port
output_mode=port
input_format=streaming
output_format=streaming
port_range=6200-6300
orchestrator_host=localhost
orchestrator_port=6000
route=/slm
script_uuid=5714ad06-a9f8-461a-b7dc-8faf07e6de6c
system_prompt="you respond conversationally"
temperature=0.7
top_p=0.9
max_tokens=150
repeat_penalty=1.0
inference_timeout=5
port=6201
data_port=6001
